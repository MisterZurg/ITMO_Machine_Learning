## Exam questions. Machine Learning Technologies
### 1. Supervised learning (an overview of the tasks and algorithms).
### 2. Unsupervised learning (an overview of the tasks and algorithms).
### 3. Machine learning and Bayes theorem. Prior and posterior distribution.
### 4. Error decomposition. Bias and variance tradeoff.
### 5. Linear regression model and Logistic regression.
### 6. The method of the k-nearest neighbors. Support vector machines. Kernel trick.
### 7. Decision trees and principles of its construction.
### 8. Types of features. Feature selection approaches. One-hot-encoding.
### 9. Data visualization and dimension-reduction algorithms: PCA and t-SNE.
### 10. Classification metrics. Accuracy, precision, recall, F1-score, log-loss, ROC-AUC. Types of errors, confusion matrix. Metrics of accuracy for regression models.
### 11. Ensembles of algorithms. Bagging. Random Forrest.
### 12. Boosting. Gradient boosting algorithms.
### 13. Clustering algorithms. K-means and DBSCAN. Estimation of clustering quality.
### 14. Multilayer perceptron. Activation functions and loss functions in neural networks. Parameters and hyperparameters.
### 15. Training of the deep neural network as an optimization problem. Gradient descent, stochastic gradient descent, Momentum, RMSProp and Adam algorithms.
### 16. Deep multi-layer neural networks. Backpropagation algorithm. The problem of vanishing and exploding gradients and the methods of its solution.
### 17. Datasets: train, test, validation (dev) sets. Cross-validation. Monitoring the learning process. Overfitting.
### 18. Convolutional neural networks (CNNs): convolution, pooling, padding, feature maps, low-level and high-level features. 
### 19. Transfer learning approach. An overview of modern CNN architectures and open-source datasets. Advantages and disadvantages of modern CNNs.
### 20. Natural language processing. Bag of words approach. TF-IDF method. Stemming and lemmatization. Stop words.
### 21. Word embeddings. Skip-gram model. Word2vec, Glove, BERT.
### 22. Sequence analysis tasks. Simple recurrent neural network architecture.
### 23. LSTM and GRU cells. Memory in neural networks.
### 24. Autoencoders and representation learning. Latent Space.
### 25. Neural Style Transfer. Style and Content loss.
